{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4f0b7d6",
   "metadata": {},
   "source": [
    "# pip installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b2e557",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbaf82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425c05ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install win10toast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cc77ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7743ee00",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d1d462",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f388474",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install mediapipe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effa119b",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b45b3ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import mediapipe as mp #trial: for now we will try using mediapipe as pretrained model\n",
    "import math as m\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f1454cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using holistic model to additionally detect face landmark rather than pose only\n",
    "#if it turns out too heavy, then switch holistic to pose in the future\n",
    "mp_holistic = mp.solutions.holistic\n",
    "mp_drawing = mp.solutions.drawing_utils #for drawing the landmark to the screen (opencv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c4e4e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_points(img, holisticOut, mp_holistic):\n",
    "    '''\n",
    "    Draw the detected result to opencv bgr image\n",
    "    \n",
    "    image: A three channel BGR image represented as numpy ndarray.\n",
    "    holisticOut: the detected result of the holistic model\n",
    "    \n",
    "    no return, since img.flags.writeable is assumed to be True (from the mp_predict() below)\n",
    "    '''\n",
    "    #draw all: face, pose (body), right and left hand\n",
    "    \n",
    "    #skip the face part, because we will draw from long range\n",
    "    mp_drawing.draw_landmarks(img, holisticOut.face_landmarks, mp_holistic.FACEMESH_TESSELATION,\n",
    "                             mp_drawing.DrawingSpec(color=(80,112,4), thickness=1, circle_radius=1), \n",
    "                             mp_drawing.DrawingSpec(color=(4,176,176), thickness=1, circle_radius=1)\n",
    "                             ) # face'''\n",
    "    mp_drawing.draw_landmarks(img, holisticOut.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color=(3,37,205), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(3,158,205), thickness=2, circle_radius=2)\n",
    "                             ) # pose\n",
    "    \n",
    "    #left and right hand\n",
    "    mp_drawing.draw_landmarks(img, holisticOut.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                              mp_drawing.DrawingSpec(color=(255,64,90), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(255,154,167), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    mp_drawing.draw_landmarks(img, holisticOut.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(255,123,21), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(255,192,144), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    '''\n",
    "    if(holisticOut.pose_landmarks):\n",
    "        h,w = img.shape[:2]\n",
    "        lm = holisticOut.pose_landmarks\n",
    "        lmPose = mp_holistic.PoseLandmark\n",
    "        cv2.line(img, (int(lm.landmark[lmPose.LEFT_SHOULDER].x * w), int(lm.landmark[lmPose.LEFT_SHOULDER].y * h)), (int(lm.landmark[lmPose.LEFT_EAR].x * w), int(lm.landmark[lmPose.LEFT_EAR].y * h)), (3,158,205), 2)\n",
    "        #cv2.line(img, (int(lm.landmark[lmPose.LEFT_SHOULDER].x * w), int(lm.landmark[lmPose.LEFT_SHOULDER].y * h)), (int(lm.landmark[lmPose.LEFT_SHOULDER].x * w), int(lm.landmark[lmPose.LEFT_SHOULDER].y * h) - 100), (3,158,205), 2)\n",
    "        #cv2.line(img, (int(lm.landmark[lmPose.LEFT_HIP].x * w), int(lm.landmark[lmPose.LEFT_HIP].y * h)), (int(lm.landmark[lmPose.LEFT_SHOULDER].x * w), int(lm.landmark[lmPose.LEFT_SHOULDER].y * h)), (3,158,205), 2)\n",
    "        #cv2.line(img, (int(lm.landmark[lmPose.LEFT_HIP].x * w), int(lm.landmark[lmPose.LEFT_HIP].y * h)), (int(lm.landmark[lmPose.LEFT_HIP].x * w), int(lm.landmark[lmPose.LEFT_HIP].y * h) - 100), (3,158,205), 2)\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a115f67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mp_predict(img, holistic):\n",
    "    '''\n",
    "    launched one cycle of holistic prediction in mediapipe\n",
    "    \n",
    "    image: A three channel BGR image represented as numpy ndarray.\n",
    "    holistic: model \n",
    "    '''\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) #mediapipe works in rgb, convert first\n",
    "    \n",
    "    #beware! Image in Opencv is passed by reference, any modification to the data will change it\n",
    "    #use img.flags.writeable = False to turn off\n",
    "    img.flags.writeable = False                  \n",
    "    holisticOut = holistic.process(img)                   # Make prediction, requires rgb\n",
    "    img.flags.writeable = True\n",
    "    \n",
    "    #convert back\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "    return img, holisticOut #holisticOut will be passed to the draw_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c8dee9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper functions\n",
    "def findDistance(x1,y1,x2,y2):\n",
    "    dist = m.sqrt((x2-x1)**2 + (y2-y1)**2)\n",
    "    return dist\n",
    "def findAngle(x1, y1, x2, y2):\n",
    "    theta = m.acos( (y2 -y1)*(-y1) / (m.sqrt((x2 - x1)**2 + (y2 - y1)**2 ) * y1))\n",
    "    #degree = int(180/m.pi)*theta\n",
    "    #return degree\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f46b074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "width = 640.0\n",
      "height = 480.0\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0) #create opencv capture object\n",
    "\n",
    "holistic_params = {\n",
    "    'min_detection_confidence' : 0.5, \n",
    "    'min_tracking_confidence': 0.5\n",
    "}\n",
    "firstTime = True\n",
    "with mp_holistic.Holistic(min_tracking_confidence=holistic_params['min_tracking_confidence'], min_detection_confidence=holistic_params['min_detection_confidence']) as holistic:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read() #read one image in webcam\n",
    "        #print(frame.shape)\n",
    "        if firstTime:\n",
    "            print(f'width = {cap.get(3)}')\n",
    "            print(f'height = {cap.get(4)}')\n",
    "            firstTime = False\n",
    "        # begin detecting pipeline\n",
    "        img, holisticOut = mp_predict(frame, holistic)\n",
    "        draw_points(img, holisticOut, mp_holistic)\n",
    "\n",
    "        cv2.imshow('Holistic Predictions', img) #return to OpenCV Drawing window\n",
    "        \n",
    "        # stop opencv\n",
    "        if cv2.waitKey(10) == ord('q'):\n",
    "            break\n",
    "            \n",
    "    #opencv release pipeline\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c53770c",
   "metadata": {},
   "source": [
    "#checking how many points are ther ein holisticOut\n",
    "print(len(holisticOut.face_landmarks.landmark)) #468 points\n",
    "print(len(holisticOut.pose_landmarks.landmark)) #33 points\n",
    "#print(len(holisticOut.left_hand_landmarks)) #21 points\n",
    "print(len(holisticOut.right_hand_landmarks.landmark)) #21 points\n",
    "\n",
    "#Beware! If it doesn't detect any, return shape will be None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fe83cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_point_data(holisticOut, mp_holistic, h, w):\n",
    "    #will be in order of [pose, face, left_hand, right_hand]\n",
    "    #face  468 * [x,y,z]\n",
    "    #pose 33 * [x,y,z,visibility]\n",
    "    #left hand right hand 21 [x,y,z]\n",
    "    pose_data = np.array([[hOut.x, hOut.y, hOut.z, hOut.visibility] for hOut in holisticOut.pose_landmarks.landmark]).flatten() if holisticOut.pose_landmarks else np.zeros(33*4)\n",
    "    face_data = np.array([[hOut.x, hOut.y, hOut.z] for hOut in holisticOut.face_landmarks.landmark]).flatten() if holisticOut.face_landmarks else np.zeros(468*3)\n",
    "    lh_data = np.array([[hOut.x, hOut.y, hOut.z] for hOut in holisticOut.left_hand_landmarks.landmark]).flatten() if holisticOut.left_hand_landmarks else np.zeros(21*3)\n",
    "    rh_data = np.array([[hOut.x, hOut.y, hOut.z] for hOut in holisticOut.right_hand_landmarks.landmark]).flatten() if holisticOut.right_hand_landmarks else np.zeros(21*3)\n",
    "    lm = holisticOut.pose_landmarks\n",
    "    lmPose = mp_holistic.PoseLandmark\n",
    "    neck_angle_data = np.array([findAngle(int(lm.landmark[lmPose.LEFT_SHOULDER].x * w), int(lm.landmark[lmPose.LEFT_SHOULDER].y * h), int(lm.landmark[lmPose.LEFT_EAR].x * w), int(lm.landmark[lmPose.LEFT_EAR].y * h))]) if holisticOut.pose_landmarks else np.zeros(1)\n",
    "    return np.concatenate([pose_data, face_data, lh_data, rh_data])\n",
    "    #return np.concatenate([pose_data, lh_data, rh_data, neck_angle_data])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7617de43",
   "metadata": {},
   "source": [
    "## Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4eadf64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets\n"
     ]
    }
   ],
   "source": [
    "#BodyPostureDetection\n",
    "actions = np.array(['good', 'hand_on_cheek', 'hand_on_forehead'])\n",
    "# Path for exported data, numpy arrays\n",
    "datasets_origin_path = os.path.join('Datasets') \n",
    "# for amount of data collection, we collect 30 videos per actions, and we have 30 frame per videos\n",
    "number_of_videos = 250\n",
    "frame_per_videos = 30\n",
    "print(datasets_origin_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "245c4c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare folders, os.mkdir cant create multilevel directory, so make one at one time\n",
    "\n",
    "#start from datasets_origin_path\n",
    "if not(os.path.exists(datasets_origin_path)):\n",
    "    # create the directory you want to save to\n",
    "    os.mkdir(datasets_origin_path)\n",
    "\n",
    "#iterate from each file\n",
    "for action in actions:\n",
    "    curPath = os.path.join(datasets_origin_path, action)\n",
    "    if not(os.path.exists(curPath)):\n",
    "        os.mkdir(curPath)\n",
    "    for video_idx in range(number_of_videos):\n",
    "        curPath = os.path.join(datasets_origin_path, action, str(video_idx))\n",
    "        if not(os.path.exists(curPath)):\n",
    "            os.mkdir(curPath)\n",
    "        #for frame_idx in range(frame_per_videos):\n",
    "            #curPath = os.path.join(datasets_origin_path, action, str(video_idx), str(frame_idx))\n",
    "            #if not(os.path.exists(curPath)):\n",
    "                #os.mkdir(curPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a76eeec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenamingstart = 250\n",
    "#prepare folders, os.mkdir cant create multilevel directory, so make one at one time\n",
    "\n",
    "#start from datasets_origin_path\n",
    "if not(os.path.exists(datasets_origin_path)):\n",
    "    # create the directory you want to save to\n",
    "    os.mkdir(datasets_origin_path)\n",
    "\n",
    "#iterate from each file\n",
    "for action in actions:\n",
    "    curPath = os.path.join(datasets_origin_path, action)\n",
    "    if not(os.path.exists(curPath)):\n",
    "        os.mkdir(curPath)\n",
    "    for video_idx in range(number_of_videos):\n",
    "        curPath = os.path.join(datasets_origin_path, action, str(video_idx+filenamingstart))\n",
    "        if not(os.path.exists(curPath)):\n",
    "            os.mkdir(curPath)\n",
    "        #for frame_idx in range(frame_per_videos):\n",
    "            #curPath = os.path.join(datasets_origin_path, action, str(video_idx), str(frame_idx))\n",
    "            #if not(os.path.exists(curPath)):\n",
    "                #os.mkdir(curPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f81e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0) \n",
    "customBreak = False\n",
    "from win10toast import ToastNotifier\n",
    "toast = ToastNotifier()\n",
    "\n",
    "framerate = 4410\n",
    "play_time_seconds = 1\n",
    "\n",
    "with mp_holistic.Holistic(min_detection_confidence=holistic_params['min_detection_confidence'], min_tracking_confidence=holistic_params['min_tracking_confidence']) as holistic:\n",
    "    for action in actions:\n",
    "        #videos and frames start counting from 0\n",
    "        for video_idx in range(number_of_videos):#number of videos\n",
    "            for frame_idx in range(frame_per_videos):#number of frames per videos\n",
    "                ret, frame = cap.read() #read one image in webcam\n",
    "                img, holisticOut = mp_predict(frame, holistic)\n",
    "                draw_points(img, holisticOut, mp_holistic)\n",
    "                '''\n",
    "                format for putText(...):\n",
    "                    img: bgr np.array, opencv image\n",
    "                    txt: what text to print to the screen\n",
    "                    org: coordinates of the bottom-left corner of the text string in the image: (X, Y).\n",
    "                    font: font enumerations\n",
    "                    fontScale: Font scale factor that is multiplied by the font-specific base size.\n",
    "                    color: It is the color of text string to be drawn. BGR.\n",
    "                    thickness: It is the thickness of the line in px.\n",
    "                    lineType: methods of text printing: use antialiased line to reduce pixelation\n",
    "                '''\n",
    "                #print instruction to the screen\n",
    "                if frame_idx == 0:\n",
    "                    toast.show_toast('Train done', 'done!', duration = 2, threaded = True)\n",
    "                    cv2.putText(img, f'STARTING COLLECTION {action}', (90,200), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255, 0), 4, cv2.LINE_AA)\n",
    "                    cv2.putText(img, f'Current Action: {action}, Video Number: {video_idx}', (15,12), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                    \n",
    "                    cv2.imshow('Data Collection Process', img)\n",
    "                    cv2.waitKey(4000)\n",
    "                    ret, frame = cap.read() #read one image in webcam\n",
    "                    img, holisticOut = mp_predict(frame, holistic)\n",
    "                    draw_points(img, holisticOut, mp_holistic)\n",
    "                else: \n",
    "                    cv2.putText(img, f'Current Action: {action}, Video Number: {video_idx}', (15,12), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                    cv2.imshow('Data Collection Process', img)\n",
    "                \n",
    "                #saving data\n",
    "                h,w = img.shape[:2]\n",
    "                np_point_data = extract_point_data(holisticOut, mp_holistic, h, w) #np array that has been flattened for result\n",
    "                full_path = os.path.join(datasets_origin_path, action, str(video_idx), str(frame_idx))\n",
    "                #print(np_point_data)\n",
    "                np.save(full_path, np_point_data) #syntax: np.save(path_for_file, np_array)\n",
    "\n",
    "                # stop opencv\n",
    "                if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                    customBreak = True\n",
    "                    break\n",
    "            if customBreak:\n",
    "                break\n",
    "        if customBreak:\n",
    "            break\n",
    "                    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "13c5ba0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0) \n",
    "customBreak = False\n",
    "from win10toast import ToastNotifier\n",
    "toast = ToastNotifier()\n",
    "filenamingstart = 250\n",
    "framerate = 4410\n",
    "play_time_seconds = 1\n",
    "with mp_holistic.Holistic(min_detection_confidence=holistic_params['min_detection_confidence'], min_tracking_confidence=holistic_params['min_tracking_confidence']) as holistic:\n",
    "    action = actions[2]\n",
    "    toast.show_toast('Train begin', f'{action}', duration = 2, threaded = True)\n",
    "    cv2.waitKey(4000)\n",
    "    #videos and frames start counting from 0\n",
    "    for video_idx in range(number_of_videos):#number of videos\n",
    "        for frame_idx in range(frame_per_videos):#number of frames per videos\n",
    "            ret, frame = cap.read() #read one image in webcam\n",
    "            img, holisticOut = mp_predict(frame, holistic)\n",
    "            draw_points(img, holisticOut, mp_holistic)\n",
    "            '''\n",
    "            format for putText(...):\n",
    "                img: bgr np.array, opencv image\n",
    "                txt: what text to print to the screen\n",
    "                org: coordinates of the bottom-left corner of the text string in the image: (X, Y).\n",
    "                font: font enumerations\n",
    "                fontScale: Font scale factor that is multiplied by the font-specific base size.\n",
    "                color: It is the color of text string to be drawn. BGR.\n",
    "                thickness: It is the thickness of the line in px.\n",
    "                lineType: methods of text printing: use antialiased line to reduce pixelation\n",
    "            '''\n",
    "            #print instruction to the screen\n",
    "            '''\n",
    "            if frame_idx == 0:\n",
    "                toast.show_toast('Train done', 'done!', duration = 2, threaded = True)\n",
    "                cv2.putText(img, f'STARTING COLLECTION {action}', (90,200), \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255, 0), 4, cv2.LINE_AA)\n",
    "                cv2.imshow('Data Collection Process', img)\n",
    "                cv2.waitKey(2000)\n",
    "                ret, frame = cap.read() #read one image in webcam\n",
    "                img, holisticOut = mp_predict(frame, holistic)\n",
    "                draw_points(img, holisticOut, mp_holistic)\n",
    "            '''\n",
    "            cv2.putText(img, f'Current Action: {action}, Video Number: {video_idx+filenamingstart}', (15,12), \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "            cv2.imshow('Data Collection Process', img)\n",
    "            #saving data\n",
    "            h,w = img.shape[:2]\n",
    "            np_point_data = extract_point_data(holisticOut, mp_holistic, h, w) #np array that has been flattened for result\n",
    "            full_path = os.path.join(datasets_origin_path, action, str(video_idx+filenamingstart), str(frame_idx))\n",
    "            #print(np_point_data)\n",
    "            np.save(full_path, np_point_data) #syntax: np.save(path_for_file, np_array)\n",
    "\n",
    "            # stop opencv\n",
    "            if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                customBreak = True\n",
    "                break\n",
    "        if customBreak:\n",
    "            break\n",
    "    toast.show_toast('Train done', 'done!', duration = 2, threaded = True)         \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba6ae1e",
   "metadata": {},
   "source": [
    "# Preprocess Data, Output labelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1bb3eede",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bd09478c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'good': 0, 'hand_on_cheek': 1, 'hand_on_forehead': 2}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_map = {label:num for num, label in enumerate(actions)}\n",
    "label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f3d4b68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences, labels = [], []\n",
    "new_number_of_videos = 500\n",
    "for action in actions:\n",
    "    for sequence in range(new_number_of_videos):\n",
    "        window = []\n",
    "        for frame_idx in range(frame_per_videos):\n",
    "            res = np.load(os.path.join(datasets_origin_path, action, str(sequence), f\"{frame_idx}.npy\"))\n",
    "            window.append(res)\n",
    "        sequences.append(window)\n",
    "        labels.append(label_map[action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d5793af9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xshape = (1500, 30, 1662)\n",
      "Yshape = (1500,)\n",
      "shape of ytrain: (1350, 3)\n",
      "shape of ytest: (150, 3)\n"
     ]
    }
   ],
   "source": [
    "X = np.array(sequences)\n",
    "print(f'Xshape = {X.shape}')\n",
    "print(f'Yshape = {np.array(labels).shape}')\n",
    "y = to_categorical(labels).astype(int)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
    "print(f'shape of ytrain: {y_train.shape}')\n",
    "print(f'shape of ytest: {y_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1766525e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scaler.skl']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "#preprocess data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train.reshape(X_train.shape[0], -1)).reshape(X_train.shape)\n",
    "X_test = scaler.transform(X_test.reshape(X_test.shape[0], -1)).reshape(X_test.shape)\n",
    "joblib.dump(scaler, 'scaler.skl') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2959795",
   "metadata": {},
   "source": [
    "# Model Trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5b49dec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "from keras.callbacks import TensorBoard, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d37ea2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildModel(optimizer_params = 'Adam', loss_params = 'categorical_crossentropy', metrics_params = ['categorical_accuracy']):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(64, return_sequences=True, activation='relu', input_shape=(X.shape[1],X.shape[2])))\n",
    "    model.add(LSTM(128, return_sequences=True, activation='relu'))\n",
    "    model.add(LSTM(64, return_sequences=False, activation='relu'))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(actions.shape[0], activation='softmax'))\n",
    "    model.compile(optimizer=optimizer_params, loss=loss_params, metrics=metrics_params)\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "962236a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_6 (LSTM)               (None, 30, 64)            442112    \n",
      "                                                                 \n",
      " lstm_7 (LSTM)               (None, 30, 128)           98816     \n",
      "                                                                 \n",
      " lstm_8 (LSTM)               (None, 64)                49408     \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 3)                 99        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 596,675\n",
      "Trainable params: 596,675\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/150\n",
      "43/43 [==============================] - 6s 59ms/step - loss: 6.1238 - categorical_accuracy: 0.4022 - val_loss: 7.6334 - val_categorical_accuracy: 0.4800\n",
      "Epoch 2/150\n",
      "43/43 [==============================] - 2s 49ms/step - loss: 10.0023 - categorical_accuracy: 0.4689 - val_loss: 14.0347 - val_categorical_accuracy: 0.4533\n",
      "Epoch 3/150\n",
      "43/43 [==============================] - 2s 50ms/step - loss: 12.0126 - categorical_accuracy: 0.5200 - val_loss: 7.7563 - val_categorical_accuracy: 0.5467\n",
      "Epoch 4/150\n",
      "43/43 [==============================] - 2s 50ms/step - loss: 7.6206 - categorical_accuracy: 0.5644 - val_loss: 19.0288 - val_categorical_accuracy: 0.4467\n",
      "Epoch 5/150\n",
      "43/43 [==============================] - 2s 48ms/step - loss: 75.3615 - categorical_accuracy: 0.4533 - val_loss: 46.4945 - val_categorical_accuracy: 0.4933\n",
      "Epoch 6/150\n",
      "43/43 [==============================] - 2s 48ms/step - loss: 235.7571 - categorical_accuracy: 0.4163 - val_loss: 482.9925 - val_categorical_accuracy: 0.4000\n",
      "Epoch 7/150\n",
      "43/43 [==============================] - 2s 48ms/step - loss: 298.4830 - categorical_accuracy: 0.3763 - val_loss: 119.6923 - val_categorical_accuracy: 0.3333\n",
      "Epoch 8/150\n",
      "43/43 [==============================] - 2s 48ms/step - loss: 32.4159 - categorical_accuracy: 0.3822 - val_loss: 2.8815 - val_categorical_accuracy: 0.4067\n",
      "Epoch 9/150\n",
      "43/43 [==============================] - 2s 48ms/step - loss: 2.7543 - categorical_accuracy: 0.4363 - val_loss: 1.5205 - val_categorical_accuracy: 0.4333\n",
      "Epoch 10/150\n",
      "43/43 [==============================] - 2s 48ms/step - loss: 1.9022 - categorical_accuracy: 0.4711 - val_loss: 2.5463 - val_categorical_accuracy: 0.5133\n",
      "Epoch 11/150\n",
      "43/43 [==============================] - 2s 49ms/step - loss: 1.7060 - categorical_accuracy: 0.5526 - val_loss: 0.9890 - val_categorical_accuracy: 0.5533\n",
      "Epoch 12/150\n",
      "43/43 [==============================] - 2s 47ms/step - loss: 2.9173 - categorical_accuracy: 0.5778 - val_loss: 1.0562 - val_categorical_accuracy: 0.5467\n",
      "Epoch 13/150\n",
      "43/43 [==============================] - 2s 47ms/step - loss: 0.9402 - categorical_accuracy: 0.5852 - val_loss: 0.9995 - val_categorical_accuracy: 0.5400\n",
      "Epoch 14/150\n",
      "43/43 [==============================] - 2s 48ms/step - loss: 0.9816 - categorical_accuracy: 0.6052 - val_loss: 4.2703 - val_categorical_accuracy: 0.5267\n",
      "Epoch 15/150\n",
      "43/43 [==============================] - 2s 48ms/step - loss: 1.0670 - categorical_accuracy: 0.5356 - val_loss: 1.0499 - val_categorical_accuracy: 0.4667\n",
      "Epoch 16/150\n",
      "43/43 [==============================] - 2s 48ms/step - loss: 1.0031 - categorical_accuracy: 0.5193 - val_loss: 1.6282 - val_categorical_accuracy: 0.3800\n",
      "Epoch 17/150\n",
      "43/43 [==============================] - 2s 48ms/step - loss: 1.1804 - categorical_accuracy: 0.5519 - val_loss: 1.1060 - val_categorical_accuracy: 0.4800\n",
      "Epoch 18/150\n",
      "43/43 [==============================] - 2s 48ms/step - loss: 1.6366 - categorical_accuracy: 0.5978 - val_loss: 1.0253 - val_categorical_accuracy: 0.5533\n",
      "Epoch 19/150\n",
      "43/43 [==============================] - 2s 48ms/step - loss: 1.1056 - categorical_accuracy: 0.6015 - val_loss: 0.8537 - val_categorical_accuracy: 0.5933\n",
      "Epoch 20/150\n",
      "43/43 [==============================] - 2s 48ms/step - loss: 0.8532 - categorical_accuracy: 0.6030 - val_loss: 0.8905 - val_categorical_accuracy: 0.5133\n",
      "Epoch 21/150\n",
      "43/43 [==============================] - 2s 48ms/step - loss: 1.2612 - categorical_accuracy: 0.6326 - val_loss: 0.8571 - val_categorical_accuracy: 0.6267\n",
      "Epoch 22/150\n",
      "43/43 [==============================] - 2s 48ms/step - loss: 0.8737 - categorical_accuracy: 0.6526 - val_loss: 2.4692 - val_categorical_accuracy: 0.4067\n",
      "Epoch 23/150\n",
      "43/43 [==============================] - 2s 48ms/step - loss: 4.1324 - categorical_accuracy: 0.3681 - val_loss: 2.4092 - val_categorical_accuracy: 0.3933\n",
      "Epoch 24/150\n",
      "43/43 [==============================] - 2s 48ms/step - loss: 2.1176 - categorical_accuracy: 0.4993 - val_loss: 1.4332 - val_categorical_accuracy: 0.4133\n",
      "Epoch 25/150\n",
      "43/43 [==============================] - 2s 48ms/step - loss: 2.0921 - categorical_accuracy: 0.4807 - val_loss: 1.6034 - val_categorical_accuracy: 0.4067\n",
      "Epoch 26/150\n",
      "43/43 [==============================] - 2s 49ms/step - loss: 1.8565 - categorical_accuracy: 0.4607 - val_loss: 3.3615 - val_categorical_accuracy: 0.3733\n",
      "Epoch 27/150\n",
      "43/43 [==============================] - 2s 49ms/step - loss: 2.7648 - categorical_accuracy: 0.4178 - val_loss: 1.4955 - val_categorical_accuracy: 0.4267\n",
      "Epoch 28/150\n",
      "43/43 [==============================] - 2s 48ms/step - loss: 1.5634 - categorical_accuracy: 0.4578 - val_loss: 1.7257 - val_categorical_accuracy: 0.4133\n",
      "Epoch 29/150\n",
      "43/43 [==============================] - 2s 49ms/step - loss: 1.8161 - categorical_accuracy: 0.4911 - val_loss: 0.9997 - val_categorical_accuracy: 0.5200\n",
      "Epoch 30/150\n",
      "43/43 [==============================] - 2s 48ms/step - loss: 1.4257 - categorical_accuracy: 0.4963 - val_loss: 0.9432 - val_categorical_accuracy: 0.4933\n",
      "Epoch 31/150\n",
      "43/43 [==============================] - 2s 48ms/step - loss: 1.0488 - categorical_accuracy: 0.5044 - val_loss: 0.9898 - val_categorical_accuracy: 0.5000\n",
      "Epoch 32/150\n",
      "43/43 [==============================] - 2s 48ms/step - loss: 1.0017 - categorical_accuracy: 0.5274 - val_loss: 0.9387 - val_categorical_accuracy: 0.4800\n",
      "Epoch 33/150\n",
      "43/43 [==============================] - 2s 48ms/step - loss: 1.0125 - categorical_accuracy: 0.5385 - val_loss: 0.9241 - val_categorical_accuracy: 0.5467\n",
      "Epoch 34/150\n",
      "43/43 [==============================] - 2s 48ms/step - loss: 0.8886 - categorical_accuracy: 0.6037 - val_loss: 0.8362 - val_categorical_accuracy: 0.6533\n",
      "Epoch 35/150\n",
      "43/43 [==============================] - 2s 48ms/step - loss: 0.8353 - categorical_accuracy: 0.6526 - val_loss: 0.7958 - val_categorical_accuracy: 0.6533\n",
      "Epoch 36/150\n",
      "43/43 [==============================] - 2s 48ms/step - loss: 0.8782 - categorical_accuracy: 0.6474 - val_loss: 0.7797 - val_categorical_accuracy: 0.6667\n",
      "Epoch 37/150\n",
      "43/43 [==============================] - 2s 48ms/step - loss: 0.8406 - categorical_accuracy: 0.6748 - val_loss: 0.7423 - val_categorical_accuracy: 0.6933\n",
      "Epoch 38/150\n",
      "43/43 [==============================] - 2s 48ms/step - loss: 0.7582 - categorical_accuracy: 0.6948 - val_loss: 0.7208 - val_categorical_accuracy: 0.7067\n",
      "Epoch 39/150\n",
      "43/43 [==============================] - 2s 48ms/step - loss: 0.7239 - categorical_accuracy: 0.6911 - val_loss: 0.7040 - val_categorical_accuracy: 0.6933\n",
      "Epoch 40/150\n",
      "43/43 [==============================] - 2s 48ms/step - loss: 0.7468 - categorical_accuracy: 0.6926 - val_loss: 0.7143 - val_categorical_accuracy: 0.6867\n",
      "Epoch 41/150\n",
      "43/43 [==============================] - 2s 48ms/step - loss: 0.7677 - categorical_accuracy: 0.6867 - val_loss: 0.7117 - val_categorical_accuracy: 0.6800\n",
      "Epoch 42/150\n",
      "43/43 [==============================] - 2s 48ms/step - loss: 0.7036 - categorical_accuracy: 0.6763 - val_loss: 0.7232 - val_categorical_accuracy: 0.6933\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/150\n",
      "43/43 [==============================] - 2s 48ms/step - loss: 0.6915 - categorical_accuracy: 0.6837 - val_loss: 0.6819 - val_categorical_accuracy: 0.7000\n",
      "Epoch 44/150\n",
      "43/43 [==============================] - 2s 49ms/step - loss: 0.7358 - categorical_accuracy: 0.6985 - val_loss: 0.7005 - val_categorical_accuracy: 0.7000\n",
      "Epoch 45/150\n",
      "43/43 [==============================] - 2s 49ms/step - loss: 0.7382 - categorical_accuracy: 0.7156 - val_loss: 0.7359 - val_categorical_accuracy: 0.6867\n",
      "Epoch 46/150\n",
      "43/43 [==============================] - 2s 49ms/step - loss: 0.9642 - categorical_accuracy: 0.7267 - val_loss: 0.6372 - val_categorical_accuracy: 0.7400\n",
      "Epoch 47/150\n",
      "43/43 [==============================] - 2s 48ms/step - loss: 0.6792 - categorical_accuracy: 0.7578 - val_loss: 0.6345 - val_categorical_accuracy: 0.7467\n",
      "Epoch 48/150\n",
      "43/43 [==============================] - 2s 48ms/step - loss: 0.6292 - categorical_accuracy: 0.7659 - val_loss: 0.5910 - val_categorical_accuracy: 0.7733\n",
      "Epoch 49/150\n",
      "43/43 [==============================] - 2s 48ms/step - loss: 0.6257 - categorical_accuracy: 0.7704 - val_loss: 0.6219 - val_categorical_accuracy: 0.7667\n",
      "Epoch 50/150\n",
      "43/43 [==============================] - 2s 48ms/step - loss: 0.6183 - categorical_accuracy: 0.7822 - val_loss: 0.5655 - val_categorical_accuracy: 0.7733\n",
      "Epoch 51/150\n",
      "43/43 [==============================] - 2s 48ms/step - loss: 0.6435 - categorical_accuracy: 0.7659 - val_loss: 0.5958 - val_categorical_accuracy: 0.7333\n",
      "Epoch 52/150\n",
      "43/43 [==============================] - 2s 48ms/step - loss: 0.6207 - categorical_accuracy: 0.7519 - val_loss: 0.5967 - val_categorical_accuracy: 0.7333\n",
      "Epoch 53/150\n",
      "43/43 [==============================] - 2s 48ms/step - loss: 0.6439 - categorical_accuracy: 0.7526 - val_loss: 0.6722 - val_categorical_accuracy: 0.7133\n",
      "Epoch 54/150\n",
      "43/43 [==============================] - 2s 49ms/step - loss: 0.5980 - categorical_accuracy: 0.7793 - val_loss: 0.5507 - val_categorical_accuracy: 0.7600\n",
      "Epoch 55/150\n",
      "43/43 [==============================] - 2s 47ms/step - loss: 0.7816 - categorical_accuracy: 0.7207 - val_loss: 1.7690 - val_categorical_accuracy: 0.6000\n",
      "Epoch 56/150\n",
      "43/43 [==============================] - 2s 49ms/step - loss: 0.7860 - categorical_accuracy: 0.6867 - val_loss: 0.8884 - val_categorical_accuracy: 0.6267\n",
      "Epoch 57/150\n",
      "43/43 [==============================] - 2s 49ms/step - loss: 0.8251 - categorical_accuracy: 0.6763 - val_loss: 0.7999 - val_categorical_accuracy: 0.6000\n",
      "Epoch 58/150\n",
      "43/43 [==============================] - 2s 49ms/step - loss: 0.8298 - categorical_accuracy: 0.6630 - val_loss: 0.8420 - val_categorical_accuracy: 0.6000\n",
      "Epoch 59/150\n",
      "43/43 [==============================] - 2s 48ms/step - loss: 0.7902 - categorical_accuracy: 0.6763 - val_loss: 0.8277 - val_categorical_accuracy: 0.6000\n",
      "Epoch 60/150\n",
      "43/43 [==============================] - 2s 49ms/step - loss: 0.7633 - categorical_accuracy: 0.6852 - val_loss: 0.7880 - val_categorical_accuracy: 0.6133\n",
      "Epoch 61/150\n",
      "43/43 [==============================] - 2s 49ms/step - loss: 0.7801 - categorical_accuracy: 0.7015 - val_loss: 0.8204 - val_categorical_accuracy: 0.6133\n",
      "Epoch 62/150\n",
      "43/43 [==============================] - 2s 50ms/step - loss: 0.7522 - categorical_accuracy: 0.7059 - val_loss: 0.7334 - val_categorical_accuracy: 0.6667\n",
      "Epoch 63/150\n",
      "43/43 [==============================] - 2s 48ms/step - loss: 0.6762 - categorical_accuracy: 0.7296 - val_loss: 0.6757 - val_categorical_accuracy: 0.6667\n",
      "Epoch 64/150\n",
      "43/43 [==============================] - 2s 48ms/step - loss: 0.7144 - categorical_accuracy: 0.7333 - val_loss: 0.7749 - val_categorical_accuracy: 0.6600\n",
      "Epoch 65/150\n",
      "43/43 [==============================] - 2s 48ms/step - loss: 0.6751 - categorical_accuracy: 0.7622 - val_loss: 0.5598 - val_categorical_accuracy: 0.7933\n",
      "Epoch 66/150\n",
      "43/43 [==============================] - 2s 48ms/step - loss: 0.5058 - categorical_accuracy: 0.8341 - val_loss: 0.5232 - val_categorical_accuracy: 0.7933\n",
      "Epoch 67/150\n",
      "43/43 [==============================] - 2s 49ms/step - loss: 0.7074 - categorical_accuracy: 0.8289 - val_loss: 0.4521 - val_categorical_accuracy: 0.8133\n",
      "Epoch 68/150\n",
      "43/43 [==============================] - 2s 48ms/step - loss: 1.9902 - categorical_accuracy: 0.7067 - val_loss: 0.8607 - val_categorical_accuracy: 0.5867\n",
      "Epoch 69/150\n",
      "43/43 [==============================] - 2s 48ms/step - loss: 1.9079 - categorical_accuracy: 0.6600 - val_loss: 25.9212 - val_categorical_accuracy: 0.5133\n",
      "Epoch 70/150\n",
      "43/43 [==============================] - 2s 48ms/step - loss: 130.8707 - categorical_accuracy: 0.3889 - val_loss: 3.7990 - val_categorical_accuracy: 0.4867\n",
      "Epoch 71/150\n",
      "43/43 [==============================] - 2s 49ms/step - loss: 2.2862 - categorical_accuracy: 0.5556 - val_loss: 2.1020 - val_categorical_accuracy: 0.5600\n",
      "Epoch 72/150\n",
      "43/43 [==============================] - 2s 49ms/step - loss: 1.1725 - categorical_accuracy: 0.6074 - val_loss: 1.3192 - val_categorical_accuracy: 0.5533\n",
      "Epoch 73/150\n",
      "43/43 [==============================] - 2s 48ms/step - loss: 0.8190 - categorical_accuracy: 0.6437 - val_loss: 1.2476 - val_categorical_accuracy: 0.6067\n",
      "Epoch 74/150\n",
      "43/43 [==============================] - 2s 49ms/step - loss: 0.7964 - categorical_accuracy: 0.6763 - val_loss: 1.1133 - val_categorical_accuracy: 0.6733\n",
      "Epoch 75/150\n",
      "43/43 [==============================] - 2s 48ms/step - loss: 0.8092 - categorical_accuracy: 0.7007 - val_loss: 1.0787 - val_categorical_accuracy: 0.7200\n",
      "Epoch 76/150\n",
      "43/43 [==============================] - 2s 48ms/step - loss: 0.7006 - categorical_accuracy: 0.7533 - val_loss: 0.6753 - val_categorical_accuracy: 0.7000\n",
      "Epoch 77/150\n",
      "43/43 [==============================] - 2s 48ms/step - loss: 1.5330 - categorical_accuracy: 0.7911 - val_loss: 0.9299 - val_categorical_accuracy: 0.7667\n",
      "Epoch 78/150\n",
      "43/43 [==============================] - 2s 48ms/step - loss: 0.5327 - categorical_accuracy: 0.8296 - val_loss: 0.8524 - val_categorical_accuracy: 0.8200\n",
      "Epoch 79/150\n",
      "43/43 [==============================] - 2s 48ms/step - loss: 0.4930 - categorical_accuracy: 0.8370 - val_loss: 0.8282 - val_categorical_accuracy: 0.8467\n",
      "Epoch 80/150\n",
      "43/43 [==============================] - 2s 48ms/step - loss: 0.4449 - categorical_accuracy: 0.8570 - val_loss: 0.8073 - val_categorical_accuracy: 0.8467\n",
      "Epoch 81/150\n",
      "43/43 [==============================] - 2s 48ms/step - loss: 0.4416 - categorical_accuracy: 0.8637 - val_loss: 0.7971 - val_categorical_accuracy: 0.8333\n",
      "Epoch 82/150\n",
      "43/43 [==============================] - 2s 48ms/step - loss: 0.4295 - categorical_accuracy: 0.8719 - val_loss: 0.7318 - val_categorical_accuracy: 0.8600\n",
      "Epoch 83/150\n",
      "43/43 [==============================] - 2s 49ms/step - loss: 0.3858 - categorical_accuracy: 0.8896 - val_loss: 0.7603 - val_categorical_accuracy: 0.8333\n",
      "Epoch 84/150\n",
      "43/43 [==============================] - 2s 48ms/step - loss: 0.3903 - categorical_accuracy: 0.8867 - val_loss: 0.7622 - val_categorical_accuracy: 0.8400\n",
      "Epoch 85/150\n",
      "43/43 [==============================] - 2s 48ms/step - loss: 0.3732 - categorical_accuracy: 0.8911 - val_loss: 0.7303 - val_categorical_accuracy: 0.8533\n",
      "Epoch 86/150\n",
      "43/43 [==============================] - 2s 46ms/step - loss: 0.3550 - categorical_accuracy: 0.8948 - val_loss: 0.7174 - val_categorical_accuracy: 0.8600\n",
      "Epoch 87/150\n",
      "43/43 [==============================] - 2s 48ms/step - loss: 0.3393 - categorical_accuracy: 0.8970 - val_loss: 0.7888 - val_categorical_accuracy: 0.8600\n",
      "Epoch 88/150\n",
      "43/43 [==============================] - 2s 48ms/step - loss: 0.3613 - categorical_accuracy: 0.8978 - val_loss: 0.7220 - val_categorical_accuracy: 0.8667\n",
      "Epoch 89/150\n",
      "43/43 [==============================] - 2s 49ms/step - loss: 0.3350 - categorical_accuracy: 0.9037 - val_loss: 0.7523 - val_categorical_accuracy: 0.8600\n",
      "Epoch 90/150\n",
      "43/43 [==============================] - 2s 49ms/step - loss: 0.3273 - categorical_accuracy: 0.9059 - val_loss: 0.8353 - val_categorical_accuracy: 0.8467\n",
      "Epoch 91/150\n",
      "43/43 [==============================] - 2s 49ms/step - loss: 0.3528 - categorical_accuracy: 0.9007 - val_loss: 0.7520 - val_categorical_accuracy: 0.8667\n",
      "Epoch 92/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43/43 [==============================] - 2s 49ms/step - loss: 0.3450 - categorical_accuracy: 0.9044 - val_loss: 0.7034 - val_categorical_accuracy: 0.8800\n",
      "Epoch 93/150\n",
      "43/43 [==============================] - 2s 49ms/step - loss: 0.3170 - categorical_accuracy: 0.9096 - val_loss: 0.6909 - val_categorical_accuracy: 0.8933\n",
      "Epoch 94/150\n",
      "43/43 [==============================] - 2s 48ms/step - loss: 0.3114 - categorical_accuracy: 0.9111 - val_loss: 0.6947 - val_categorical_accuracy: 0.9000\n",
      "Epoch 95/150\n",
      "43/43 [==============================] - 2s 48ms/step - loss: 0.2978 - categorical_accuracy: 0.9156 - val_loss: 0.7330 - val_categorical_accuracy: 0.9000\n",
      "Epoch 96/150\n",
      "43/43 [==============================] - 2s 48ms/step - loss: 0.2848 - categorical_accuracy: 0.9200 - val_loss: 0.6717 - val_categorical_accuracy: 0.8933\n",
      "Epoch 97/150\n",
      "43/43 [==============================] - 2s 48ms/step - loss: 0.2808 - categorical_accuracy: 0.9185 - val_loss: 0.6369 - val_categorical_accuracy: 0.9000\n",
      "Epoch 98/150\n",
      "43/43 [==============================] - 2s 48ms/step - loss: 0.2940 - categorical_accuracy: 0.9156 - val_loss: 0.2686 - val_categorical_accuracy: 0.9000\n",
      "Epoch 99/150\n",
      "43/43 [==============================] - 2s 48ms/step - loss: 0.2524 - categorical_accuracy: 0.9319 - val_loss: 0.2444 - val_categorical_accuracy: 0.9067\n",
      "Epoch 100/150\n",
      "43/43 [==============================] - 2s 48ms/step - loss: 0.2274 - categorical_accuracy: 0.9356 - val_loss: 0.2379 - val_categorical_accuracy: 0.9067\n",
      "Epoch 101/150\n",
      "43/43 [==============================] - 2s 48ms/step - loss: 0.2162 - categorical_accuracy: 0.9393 - val_loss: 0.2631 - val_categorical_accuracy: 0.9000\n",
      "Epoch 102/150\n",
      "43/43 [==============================] - 2s 48ms/step - loss: 0.2028 - categorical_accuracy: 0.9415 - val_loss: 0.2511 - val_categorical_accuracy: 0.9133\n",
      "Epoch 103/150\n",
      "43/43 [==============================] - 2s 48ms/step - loss: 0.1995 - categorical_accuracy: 0.9393 - val_loss: 0.2479 - val_categorical_accuracy: 0.9067\n",
      "Epoch 104/150\n",
      "43/43 [==============================] - 2s 48ms/step - loss: 0.2041 - categorical_accuracy: 0.9400 - val_loss: 0.2349 - val_categorical_accuracy: 0.9133\n",
      "Epoch 105/150\n",
      "43/43 [==============================] - 2s 48ms/step - loss: 0.1951 - categorical_accuracy: 0.9481 - val_loss: 0.2550 - val_categorical_accuracy: 0.9333\n",
      "Epoch 106/150\n",
      "43/43 [==============================] - 2s 48ms/step - loss: 0.1648 - categorical_accuracy: 0.9548 - val_loss: 0.2291 - val_categorical_accuracy: 0.9333\n",
      "Epoch 107/150\n",
      "43/43 [==============================] - 2s 48ms/step - loss: 0.1486 - categorical_accuracy: 0.9570 - val_loss: 0.2099 - val_categorical_accuracy: 0.9400\n",
      "Epoch 108/150\n",
      "43/43 [==============================] - 2s 48ms/step - loss: 0.1598 - categorical_accuracy: 0.9600 - val_loss: 0.1937 - val_categorical_accuracy: 0.9400\n",
      "Epoch 109/150\n",
      "43/43 [==============================] - 2s 48ms/step - loss: 0.1424 - categorical_accuracy: 0.9578 - val_loss: 0.2226 - val_categorical_accuracy: 0.9267\n",
      "Epoch 110/150\n",
      "43/43 [==============================] - 2s 48ms/step - loss: 0.1271 - categorical_accuracy: 0.9622 - val_loss: 0.1810 - val_categorical_accuracy: 0.9400\n",
      "Epoch 111/150\n",
      "43/43 [==============================] - 2s 48ms/step - loss: 0.1216 - categorical_accuracy: 0.9615 - val_loss: 0.2095 - val_categorical_accuracy: 0.9333\n",
      "Epoch 112/150\n",
      "43/43 [==============================] - 2s 48ms/step - loss: 0.1194 - categorical_accuracy: 0.9630 - val_loss: 0.2199 - val_categorical_accuracy: 0.9200\n",
      "Epoch 113/150\n",
      "43/43 [==============================] - 2s 48ms/step - loss: 0.1109 - categorical_accuracy: 0.9667 - val_loss: 0.1862 - val_categorical_accuracy: 0.9333\n",
      "Epoch 114/150\n",
      "43/43 [==============================] - 2s 48ms/step - loss: 0.1101 - categorical_accuracy: 0.9696 - val_loss: 0.1346 - val_categorical_accuracy: 0.9533\n",
      "Epoch 115/150\n",
      "43/43 [==============================] - 2s 48ms/step - loss: 0.1050 - categorical_accuracy: 0.9696 - val_loss: 0.1527 - val_categorical_accuracy: 0.9467\n",
      "Epoch 116/150\n",
      "43/43 [==============================] - 2s 48ms/step - loss: 0.0950 - categorical_accuracy: 0.9726 - val_loss: 0.1268 - val_categorical_accuracy: 0.9600\n",
      "Epoch 117/150\n",
      "43/43 [==============================] - 2s 48ms/step - loss: 0.0911 - categorical_accuracy: 0.9741 - val_loss: 0.2334 - val_categorical_accuracy: 0.9533\n",
      "Epoch 118/150\n",
      "43/43 [==============================] - 2s 48ms/step - loss: 0.1785 - categorical_accuracy: 0.9585 - val_loss: 0.2180 - val_categorical_accuracy: 0.9267\n",
      "Epoch 119/150\n",
      "43/43 [==============================] - 2s 48ms/step - loss: 0.1254 - categorical_accuracy: 0.9667 - val_loss: 0.1234 - val_categorical_accuracy: 0.9600\n",
      "Epoch 120/150\n",
      "43/43 [==============================] - 2s 49ms/step - loss: 0.0892 - categorical_accuracy: 0.9785 - val_loss: 0.1173 - val_categorical_accuracy: 0.9600\n",
      "Epoch 121/150\n",
      "43/43 [==============================] - 2s 52ms/step - loss: 0.0858 - categorical_accuracy: 0.9748 - val_loss: 0.1174 - val_categorical_accuracy: 0.9600\n",
      "Epoch 122/150\n",
      "43/43 [==============================] - 2s 51ms/step - loss: 0.8682 - categorical_accuracy: 0.9733 - val_loss: 0.2690 - val_categorical_accuracy: 0.8933\n",
      "Epoch 123/150\n",
      "43/43 [==============================] - 2s 51ms/step - loss: 0.1810 - categorical_accuracy: 0.9437 - val_loss: 0.2141 - val_categorical_accuracy: 0.9400\n",
      "Epoch 124/150\n",
      "43/43 [==============================] - 2s 51ms/step - loss: 0.1166 - categorical_accuracy: 0.9659 - val_loss: 0.1957 - val_categorical_accuracy: 0.9333\n",
      "Epoch 125/150\n",
      "43/43 [==============================] - 2s 51ms/step - loss: 0.1122 - categorical_accuracy: 0.9674 - val_loss: 0.1402 - val_categorical_accuracy: 0.9467\n",
      "Epoch 126/150\n",
      "43/43 [==============================] - 2s 51ms/step - loss: 0.0936 - categorical_accuracy: 0.9741 - val_loss: 0.1303 - val_categorical_accuracy: 0.9600\n",
      "Epoch 127/150\n",
      "43/43 [==============================] - 2s 50ms/step - loss: 0.0840 - categorical_accuracy: 0.9741 - val_loss: 0.1279 - val_categorical_accuracy: 0.9533\n",
      "Epoch 128/150\n",
      "43/43 [==============================] - 2s 50ms/step - loss: 0.0833 - categorical_accuracy: 0.9770 - val_loss: 0.1570 - val_categorical_accuracy: 0.9467\n",
      "Epoch 129/150\n",
      "43/43 [==============================] - 2s 50ms/step - loss: 0.0791 - categorical_accuracy: 0.9763 - val_loss: 0.1181 - val_categorical_accuracy: 0.9667\n",
      "Epoch 130/150\n",
      "43/43 [==============================] - 2s 50ms/step - loss: 0.0749 - categorical_accuracy: 0.9770 - val_loss: 0.1530 - val_categorical_accuracy: 0.9400\n",
      "Epoch 131/150\n",
      "43/43 [==============================] - 2s 50ms/step - loss: 0.0715 - categorical_accuracy: 0.9785 - val_loss: 0.1457 - val_categorical_accuracy: 0.9533\n",
      "Epoch 132/150\n",
      "43/43 [==============================] - 2s 50ms/step - loss: 0.0639 - categorical_accuracy: 0.9778 - val_loss: 0.1119 - val_categorical_accuracy: 0.9600\n",
      "Epoch 133/150\n",
      "43/43 [==============================] - 2s 49ms/step - loss: 0.0580 - categorical_accuracy: 0.9800 - val_loss: 0.1096 - val_categorical_accuracy: 0.9600\n",
      "Epoch 134/150\n",
      "43/43 [==============================] - 2s 50ms/step - loss: 0.0548 - categorical_accuracy: 0.9837 - val_loss: 0.1245 - val_categorical_accuracy: 0.9533\n",
      "Epoch 135/150\n",
      "43/43 [==============================] - 2s 50ms/step - loss: 0.0537 - categorical_accuracy: 0.9815 - val_loss: 0.1236 - val_categorical_accuracy: 0.9533\n",
      "Epoch 136/150\n",
      "43/43 [==============================] - 2s 50ms/step - loss: 0.0527 - categorical_accuracy: 0.9852 - val_loss: 0.1306 - val_categorical_accuracy: 0.9533\n",
      "Epoch 137/150\n",
      "43/43 [==============================] - 2s 50ms/step - loss: 0.0490 - categorical_accuracy: 0.9844 - val_loss: 0.1317 - val_categorical_accuracy: 0.9467\n",
      "Epoch 138/150\n",
      "43/43 [==============================] - 2s 51ms/step - loss: 0.0449 - categorical_accuracy: 0.9859 - val_loss: 0.1202 - val_categorical_accuracy: 0.9600\n",
      "Epoch 139/150\n",
      "43/43 [==============================] - 2s 51ms/step - loss: 0.0428 - categorical_accuracy: 0.9859 - val_loss: 0.1157 - val_categorical_accuracy: 0.9600\n",
      "Epoch 140/150\n",
      "43/43 [==============================] - 2s 51ms/step - loss: 0.0425 - categorical_accuracy: 0.9859 - val_loss: 0.1173 - val_categorical_accuracy: 0.9600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 141/150\n",
      "43/43 [==============================] - 2s 51ms/step - loss: 0.0387 - categorical_accuracy: 0.9874 - val_loss: 0.1083 - val_categorical_accuracy: 0.9667\n",
      "Epoch 142/150\n",
      "43/43 [==============================] - 2s 51ms/step - loss: 0.0373 - categorical_accuracy: 0.9881 - val_loss: 0.1100 - val_categorical_accuracy: 0.9667\n",
      "Epoch 143/150\n",
      "43/43 [==============================] - 2s 51ms/step - loss: 0.0347 - categorical_accuracy: 0.9896 - val_loss: 0.1117 - val_categorical_accuracy: 0.9667\n",
      "Epoch 144/150\n",
      "43/43 [==============================] - 2s 51ms/step - loss: 0.0349 - categorical_accuracy: 0.9881 - val_loss: 0.1126 - val_categorical_accuracy: 0.9667\n",
      "Epoch 145/150\n",
      "43/43 [==============================] - 2s 51ms/step - loss: 0.0526 - categorical_accuracy: 0.9859 - val_loss: 0.1268 - val_categorical_accuracy: 0.9533\n",
      "Epoch 146/150\n",
      "43/43 [==============================] - 2s 48ms/step - loss: 0.0548 - categorical_accuracy: 0.9837 - val_loss: 0.1204 - val_categorical_accuracy: 0.9600\n",
      "Epoch 147/150\n",
      "43/43 [==============================] - 2s 48ms/step - loss: 0.0562 - categorical_accuracy: 0.9837 - val_loss: 7.5810 - val_categorical_accuracy: 0.9067\n",
      "Epoch 148/150\n",
      "43/43 [==============================] - 2s 48ms/step - loss: 3.6372 - categorical_accuracy: 0.8948 - val_loss: 0.4227 - val_categorical_accuracy: 0.8933\n",
      "Epoch 149/150\n",
      "43/43 [==============================] - 2s 49ms/step - loss: 0.2357 - categorical_accuracy: 0.9556 - val_loss: 0.2209 - val_categorical_accuracy: 0.9400\n",
      "Epoch 150/150\n",
      "43/43 [==============================] - 2s 49ms/step - loss: 0.1348 - categorical_accuracy: 0.9733 - val_loss: 0.2840 - val_categorical_accuracy: 0.9267\n"
     ]
    }
   ],
   "source": [
    "tf.keras.utils.set_random_seed(100)\n",
    "model = buildModel()\n",
    "log_path = os.path.join('Logs')\n",
    "tensorboard = TensorBoard(log_dir=log_path)\n",
    "#es = EarlyStopping(monitor = 'val_loss', patience = 10, restore_best_weights = True)\n",
    "model.fit(X_train, y_train, epochs=150, callbacks=[tensorboard], validation_data=(X_test, y_test))\n",
    "model.save('studyface_most.h5') #save the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5fba610f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_9 (LSTM)               (None, 30, 64)            442112    \n",
      "                                                                 \n",
      " lstm_10 (LSTM)              (None, 30, 128)           98816     \n",
      "                                                                 \n",
      " lstm_11 (LSTM)              (None, 64)                49408     \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 3)                 99        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 596,675\n",
      "Trainable params: 596,675\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#load the weights\n",
    "#del model\n",
    "model = buildModel()\n",
    "#model.load_weights('studyface_more.h5')\n",
    "model.load_weights('studyface_most.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2464418",
   "metadata": {},
   "source": [
    "# Trying on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5ab045da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 19ms/step\n",
      "139\n",
      "150\n",
      "0.9266666666666666\n"
     ]
    }
   ],
   "source": [
    "#testScaler = joblib.load('scaler.skl')\n",
    "\n",
    "res = model.predict(X_test)\n",
    "ans = []\n",
    "true_ans = []\n",
    "corr = 0\n",
    "for i in range(res.shape[0]):\n",
    "    ans.append(np.argmax(res[i]))\n",
    "    true_ans.append(np.argmax(y_test[i]))\n",
    "    if(ans[i] == true_ans[i]):\n",
    "        corr+=1\n",
    "print(corr)\n",
    "print(res.shape[0])\n",
    "print(corr/res.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3601c252",
   "metadata": {},
   "source": [
    "# Test in real time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "81b1a4ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "a = [1,2,3]\n",
    "print(a[-3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3040598c",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [(245,117,16), (117,245,16), (16,117,245), (100,100,100)]\n",
    "def prob_viz(res, actions, input_frame, colors):\n",
    "    output_frame = input_frame.copy()\n",
    "    for num, prob in enumerate(res):\n",
    "        cv2.rectangle(output_frame, (0,60+num*40), (int(prob*100), 90+num*40), colors[num], -1)\n",
    "        cv2.putText(output_frame, actions[num], (0, 85+num*40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2, cv2.LINE_AA)\n",
    "        \n",
    "    return output_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c7892488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "good\n",
      "hand_on_forehead\n",
      "good\n",
      "good\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "good\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "good\n",
      "good\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_cheek\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_cheek\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "good\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "hand_on_cheek\n",
      "hand_on_forehead\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_forehead\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_forehead\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_forehead\n",
      "hand_on_cheek\n",
      "good\n",
      "hand_on_cheek\n",
      "good\n",
      "hand_on_cheek\n",
      "good\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_cheek\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "good\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_forehead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_cheek\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "good\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "good\n",
      "hand_on_cheek\n",
      "good\n",
      "good\n",
      "hand_on_cheek\n",
      "good\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "good\n",
      "good\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "good\n",
      "hand_on_cheek\n",
      "good\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "good\n",
      "good\n",
      "good\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_cheek\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "hand_on_forehead\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n"
     ]
    }
   ],
   "source": [
    "# 1. New detection variables\n",
    "sequence = []\n",
    "sentence = []\n",
    "predictions = []\n",
    "threshold = 0.5\n",
    "currentScaler = joblib.load('scaler.skl')\n",
    "cap = cv2.VideoCapture(0)\n",
    "# Set mediapipe model \n",
    "with mp_holistic.Holistic(min_detection_confidence=holistic_params['min_detection_confidence'], min_tracking_confidence=holistic_params['min_tracking_confidence']) as holistic:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read() #read one image in webcam\n",
    "        \n",
    "        img, holisticOut = mp_predict(frame, holistic)\n",
    "        draw_points(img, holisticOut, mp_holistic)\n",
    "        \n",
    "        # 2. Prediction logic\n",
    "        h,w = img.shape[:2]\n",
    "        np_point_data = extract_point_data(holisticOut, mp_holistic, h, w)\n",
    "        sequence.append(np_point_data)\n",
    "        sequence = sequence[-frame_per_videos:]\n",
    "        \n",
    "        if len(sequence) == frame_per_videos:\n",
    "            toBePredicted = np.expand_dims(sequence, axis = 0)\n",
    "            toBePredicted = currentScaler.transform(toBePredicted.reshape(toBePredicted.shape[0], -1)).reshape(toBePredicted.shape)\n",
    "            res = model.predict(toBePredicted, verbose = 0)[0] #we need to use [0] because dimens\n",
    "            #print(actions[np.argmax(res)])\n",
    "            predictions.append(np.argmax(res))\n",
    "            \n",
    "            \n",
    "        #3. Viz logic\n",
    "            if np.unique(predictions[-10:])[0]==np.argmax(res): \n",
    "                if res[np.argmax(res)] > threshold: \n",
    "                    \n",
    "                    if len(sentence) > 0: \n",
    "                        if actions[np.argmax(res)] != sentence[-1]:\n",
    "                            sentence.append(actions[np.argmax(res)])\n",
    "                    else:\n",
    "                        sentence.append(actions[np.argmax(res)])\n",
    "\n",
    "            if len(sentence) > 5: \n",
    "                sentence = sentence[-5:]\n",
    "\n",
    "            # Viz probabilities\n",
    "            img = prob_viz(res, actions, img, colors)\n",
    "            \n",
    "        cv2.rectangle(img, (0,0), (640, 40), (245, 117, 16), -1)\n",
    "        cv2.putText(img, ' '.join(sentence), (3,30), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        \n",
    "        # Show to screen\n",
    "        cv2.imshow('OpenCV Feed', img)\n",
    "\n",
    "        # Break gracefully\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cc7fbc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#not normalized one\n",
    "# 1. New detection variables\n",
    "sequence = []\n",
    "sentence = []\n",
    "predictions = []\n",
    "threshold = 0.5\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "# Set mediapipe model \n",
    "with mp_holistic.Holistic(min_detection_confidence=holistic_params['min_detection_confidence'], min_tracking_confidence=holistic_params['min_tracking_confidence']) as holistic:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read() #read one image in webcam\n",
    "        \n",
    "        img, holisticOut = mp_predict(frame, holistic)\n",
    "        draw_points(img, holisticOut, mp_holistic)\n",
    "        \n",
    "        # 2. Prediction logic\n",
    "        h,w = img.shape[:2]\n",
    "        np_point_data = extract_point_data(holisticOut, mp_holistic, h, w)\n",
    "        sequence.append(np_point_data)\n",
    "        sequence = sequence[-frame_per_videos:]\n",
    "        \n",
    "        if len(sequence) == frame_per_videos:\n",
    "            res = model.predict(np.expand_dims(sequence, axis=0), verbose = 0)[0] #we need to use [0] because dimens\n",
    "            #print(actions[np.argmax(res)])\n",
    "            predictions.append(np.argmax(res))\n",
    "            \n",
    "            \n",
    "        #3. Viz logic\n",
    "            if np.unique(predictions[-10:])[0]==np.argmax(res): \n",
    "                if res[np.argmax(res)] > threshold: \n",
    "                    \n",
    "                    if len(sentence) > 0: \n",
    "                        if actions[np.argmax(res)] != sentence[-1]:\n",
    "                            sentence.append(actions[np.argmax(res)])\n",
    "                    else:\n",
    "                        sentence.append(actions[np.argmax(res)])\n",
    "\n",
    "            if len(sentence) > 5: \n",
    "                sentence = sentence[-5:]\n",
    "\n",
    "            # Viz probabilities\n",
    "            img = prob_viz(res, actions, img, colors)\n",
    "            \n",
    "        cv2.rectangle(img, (0,0), (640, 40), (245, 117, 16), -1)\n",
    "        cv2.putText(img, ' '.join(sentence), (3,30), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        \n",
    "        # Show to screen\n",
    "        cv2.imshow('OpenCV Feed', img)\n",
    "\n",
    "        # Break gracefully\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8177bedf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e52b17c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc78177",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef32f8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
